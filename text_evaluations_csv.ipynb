{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import jellyfish\n",
    "import pandas as pd\n",
    "from jiwer import wer, cer, mer\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(dataset_csv, ann_columns, ots_columns, **kwargs):\n",
    "    \n",
    "    if os.path.isdir(dataset_csv):\n",
    "        listdir = os.listdir(dataset_csv)\n",
    "        dataset_csv = [ os.path.join(dataset_csv, d) for d in listdir if \".csv\" in d ][0]\n",
    "\n",
    "    df = pd.read_csv(dataset_csv, sep=\";\")\n",
    "    df = df.rename(columns={ann_columns: \"text_ann\", ots_columns: \"text_ots\"})\n",
    "\n",
    "    if kwargs.get('export'):\n",
    "\n",
    "        if not os.path.exists(kwargs['export']):\n",
    "            os.makedirs(kwargs['export'])\n",
    "\n",
    "        filename = f\"compiled_csv_{datetime.strftime(datetime.now(), '%Y%m%d-%H%M%S')}.csv\"\n",
    "        filename = os.path.join(kwargs['export'], filename)\n",
    "        df.to_csv(filename, sep=\";\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def evaluation_metrics(df, **kwargs):\n",
    "    df['max_len'] = df.apply(lambda x: max(tuple((len(x['text_ots']), len(x['text_ann'])))), axis=1)\n",
    "    df['levd'] = df.apply(lambda x: jellyfish.levenshtein_distance(x['text_ots'], x['text_ann']), axis=1)\n",
    "    # df['levd_wer'] = df.apply(lambda x: 1-x['levd']/len(x['text_ann']), axis=1)\n",
    "    df['levd_score'] = df.apply(lambda x: 1-x['levd']/x['max_len'], axis=1)\n",
    "\n",
    "    df['wer'] = df.apply(lambda x: 1-wer(x['text_ann'], x['text_ots']), axis=1)\n",
    "    df['cer'] = df.apply(lambda x: 1-cer(x['text_ann'], x['text_ots']), axis=1)\n",
    "    df['mer'] = df.apply(lambda x: 1-mer(x['text_ann'], x['text_ots']), axis=1)\n",
    "\n",
    "    if kwargs.get('export'):\n",
    "\n",
    "        if not os.path.exists(kwargs['export']):\n",
    "            os.makedirs(kwargs['export'])\n",
    "\n",
    "        filename = f\"eval_csv_{datetime.strftime(datetime.now(), '%Y%m%d-%H%M%S')}.csv\"\n",
    "        filename = os.path.join(kwargs['export'], filename)\n",
    "        df.to_csv(filename, sep=\";\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use:\n",
    "\n",
    "This is script for evaluating OCR extracted text againts Human annotated text as the ground truth. It uses Comma Separated Value (CSV) file input, please follow through below points to use the evaluation script. Tks.\n",
    "\n",
    "_*notes: Here we use semicolon (;) for separating each column-rows_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-easy-Steps:  \n",
    "\n",
    "- **First**, make sure you know where you store your dataset, and make sure the file format was .csv already.  \n",
    "- **Second**, define your human annotated column name on `ann_columns` and OCR extracted content on `ots_columns`, this included if you want to evaluate the result of extracted OCR with LLM-based error correction.  \n",
    "- **Third (optional)**, you can export your works by defining `export` argument on each function you'll see below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can freely define your own dataset location \n",
    "language_name = \"balinese\"\n",
    "dataset_dirname = f\"dataset/csv/{language_name}\"\n",
    "\n",
    "# after you've defined your dataset location, two important things you have to define is \n",
    "# which human annotated column (ann_columns) and which the OCR extracted text (ots_columns),\n",
    "# you can ignore the rest of the script\n",
    "compiled = read_dataset(dataset_csv=dataset_dirname, ann_columns=\"nganu\", ots_columns=\"iki\", export=\"compiled\")\n",
    "evaluate = evaluation_metrics(df=compiled, export=\"result\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ace_exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
